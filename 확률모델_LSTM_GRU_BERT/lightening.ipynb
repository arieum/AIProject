{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMe+vvtWpU6K3/PWdidX7+I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"def74396aa0a4e3d912a8c253d6da3b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f540fa8769444b2c87cd9ec620071c59","IPY_MODEL_06c163b0ec4a4cd29df2860f4deec9f5","IPY_MODEL_d9ce2819d65343d0b5e5d022b20b2fe7"],"layout":"IPY_MODEL_3bb468991c91487da3dc8bee84f70fb9"}},"f540fa8769444b2c87cd9ec620071c59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3621182a577e45babe2f829a17d0a34d","placeholder":"​","style":"IPY_MODEL_b044c67c071b4e3fb4b65839ca3e8b03","value":"Map: 100%"}},"06c163b0ec4a4cd29df2860f4deec9f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f826b111a79435fbb9ec4cab7122d6e","max":826,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3179180f665c4ec9bb58d1d6bb34ed8e","value":826}},"d9ce2819d65343d0b5e5d022b20b2fe7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04dfe4506fb745cfa08fc4b9acd1b00e","placeholder":"​","style":"IPY_MODEL_0400c32a85d74668a38ebcaec1b8e3ab","value":" 826/826 [00:00&lt;00:00, 10591.16 examples/s]"}},"3bb468991c91487da3dc8bee84f70fb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3621182a577e45babe2f829a17d0a34d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b044c67c071b4e3fb4b65839ca3e8b03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f826b111a79435fbb9ec4cab7122d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3179180f665c4ec9bb58d1d6bb34ed8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04dfe4506fb745cfa08fc4b9acd1b00e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0400c32a85d74668a38ebcaec1b8e3ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13b459545f6942af8b60f58df417772b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d1731709ef942d79a2e63ba676b96f9","IPY_MODEL_85a2478632dc4f7d84ea5f13bb4baef1","IPY_MODEL_27e3e8477f904877831ec43e2c9b1b1a"],"layout":"IPY_MODEL_ec9f760134834368a07b73e545fbd2e2"}},"2d1731709ef942d79a2e63ba676b96f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a37996aa584ba6a7b130346538de7c","placeholder":"​","style":"IPY_MODEL_8c72e2af49184f0bb227697b78b59e01","value":"Map: 100%"}},"85a2478632dc4f7d84ea5f13bb4baef1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5639c505f99f4384a15e1f1522c6651b","max":207,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6077771553d847dfb3cf6d139539a0fd","value":207}},"27e3e8477f904877831ec43e2c9b1b1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebba6a4384c541099f1c30bb1d2392e2","placeholder":"​","style":"IPY_MODEL_94dc8636a5a644dba9a6fb043f3524bb","value":" 207/207 [00:00&lt;00:00, 6710.89 examples/s]"}},"ec9f760134834368a07b73e545fbd2e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a37996aa584ba6a7b130346538de7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c72e2af49184f0bb227697b78b59e01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5639c505f99f4384a15e1f1522c6651b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6077771553d847dfb3cf6d139539a0fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebba6a4384c541099f1c30bb1d2392e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94dc8636a5a644dba9a6fb043f3524bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dea98ec2fe9a44cc875a56beb134b974":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1037670973bb4151acb6c815cc28cb30","IPY_MODEL_4eacf772aba843c79cf5f219f530b3bd","IPY_MODEL_a3c0d7637bd04c4fb761c56c4ae5b831"],"layout":"IPY_MODEL_1875d5a28e1e43ebb341f12873bcb9a9"}},"1037670973bb4151acb6c815cc28cb30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e17d9029e0b4bad8e7f283ffc3fdb39","placeholder":"​","style":"IPY_MODEL_ef681725a30b4d81a7b2ea896ec5b562","value":"Map: 100%"}},"4eacf772aba843c79cf5f219f530b3bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f29a30fc644cb49a6b79d35989b900","max":826,"min":0,"orientation":"horizontal","style":"IPY_MODEL_277ca93362024a869a15abf1b8b95ac7","value":826}},"a3c0d7637bd04c4fb761c56c4ae5b831":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0f18e51a1b844d9bc700bbeef8df473","placeholder":"​","style":"IPY_MODEL_ec643b12e18c4cb6bcb189cb62615db0","value":" 826/826 [00:00&lt;00:00, 10299.69 examples/s]"}},"1875d5a28e1e43ebb341f12873bcb9a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e17d9029e0b4bad8e7f283ffc3fdb39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef681725a30b4d81a7b2ea896ec5b562":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0f29a30fc644cb49a6b79d35989b900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"277ca93362024a869a15abf1b8b95ac7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0f18e51a1b844d9bc700bbeef8df473":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec643b12e18c4cb6bcb189cb62615db0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca096f4ccf4b4513a9af6c034d6a9a01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b39d494cdf949afad772ff205d25630","IPY_MODEL_1f6624b16a064e19bed6c9110e0d94ef","IPY_MODEL_8f694a73b8d4474eb2230a9962843575"],"layout":"IPY_MODEL_94f5daeb30994167b817b3e54d8b1d44"}},"0b39d494cdf949afad772ff205d25630":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a88c76c762e42caa9235f20b151c75c","placeholder":"​","style":"IPY_MODEL_59c567cf1473453ba86d4421ef7469b4","value":"Map: 100%"}},"1f6624b16a064e19bed6c9110e0d94ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8d62948a5594d7f943bdee821491f44","max":207,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0a986536baf406b8d11f587b3bdac45","value":207}},"8f694a73b8d4474eb2230a9962843575":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6d06baecf19481291fd4c7b7609d245","placeholder":"​","style":"IPY_MODEL_704ff93516de4842a3fa959656ebe2db","value":" 207/207 [00:00&lt;00:00, 6286.81 examples/s]"}},"94f5daeb30994167b817b3e54d8b1d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a88c76c762e42caa9235f20b151c75c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59c567cf1473453ba86d4421ef7469b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8d62948a5594d7f943bdee821491f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0a986536baf406b8d11f587b3bdac45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6d06baecf19481291fd4c7b7609d245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"704ff93516de4842a3fa959656ebe2db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b68701ea3f5346ef8e348f05a6b900fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1364764c3b64969a11b0c9b967405c8","IPY_MODEL_90821a3f257f4bf1ba54516fe88a7fee","IPY_MODEL_267aace7482642e59b5099f2585cd155"],"layout":"IPY_MODEL_c4f6b511e43a427cbd8e97d920e61b7c"}},"c1364764c3b64969a11b0c9b967405c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a084d62cfd13429091f4b842006dc560","placeholder":"​","style":"IPY_MODEL_382d584e46f9483bafef8ec03b970875","value":"Map: 100%"}},"90821a3f257f4bf1ba54516fe88a7fee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_adb21e1877a94c61841ddfb13c0d6802","max":826,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a058aae6005f413b820a8489e39b7fbd","value":826}},"267aace7482642e59b5099f2585cd155":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99139421b0cc454cb58c13186e2fcc15","placeholder":"​","style":"IPY_MODEL_dda3cf3bb8d244dbbf9a731399b130db","value":" 826/826 [00:00&lt;00:00, 7015.99 examples/s]"}},"c4f6b511e43a427cbd8e97d920e61b7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a084d62cfd13429091f4b842006dc560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"382d584e46f9483bafef8ec03b970875":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adb21e1877a94c61841ddfb13c0d6802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a058aae6005f413b820a8489e39b7fbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99139421b0cc454cb58c13186e2fcc15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dda3cf3bb8d244dbbf9a731399b130db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"226a1b69731d44d882928ed2cf2afa64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a58ebfe50e1440829bf5759d71187596","IPY_MODEL_e7d44da5c6b845eda98222532344fbfd","IPY_MODEL_8db4f07e12d24e99b2d4af48f2f8f4c7"],"layout":"IPY_MODEL_82b7801844d64cf9b982dca90d0f81a0"}},"a58ebfe50e1440829bf5759d71187596":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8b3680e4aa84219af0190f3e27d0d38","placeholder":"​","style":"IPY_MODEL_97c1900fad534530a520bfe18e1f7645","value":"Map: 100%"}},"e7d44da5c6b845eda98222532344fbfd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea98d33bf0fa4e06827f55fefe4b5959","max":207,"min":0,"orientation":"horizontal","style":"IPY_MODEL_addbd6f6ebab4810a95061c9fce4cc72","value":207}},"8db4f07e12d24e99b2d4af48f2f8f4c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c490b7bbf85841679ec21acbb830de99","placeholder":"​","style":"IPY_MODEL_93aaac1326084f14938a1918c474b1c0","value":" 207/207 [00:00&lt;00:00, 5443.97 examples/s]"}},"82b7801844d64cf9b982dca90d0f81a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8b3680e4aa84219af0190f3e27d0d38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97c1900fad534530a520bfe18e1f7645":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea98d33bf0fa4e06827f55fefe4b5959":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"addbd6f6ebab4810a95061c9fce4cc72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c490b7bbf85841679ec21acbb830de99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93aaac1326084f14938a1918c474b1c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6681465823a34b09be2dbeb4df5eebc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fd8db3e9c7e4407baf3405ace0e4f3f","IPY_MODEL_7259009ca63a4a44a8f63e52f314fa5b","IPY_MODEL_fc30261990b942199521c8563b1ebe68"],"layout":"IPY_MODEL_75b5f24372054091bf57b8c0ffa7f8f2"}},"7fd8db3e9c7e4407baf3405ace0e4f3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad94266d3c424cfe9385f49f6d9930af","placeholder":"​","style":"IPY_MODEL_c291e0e834f0495eb3b258df2bb67a28","value":"Map: 100%"}},"7259009ca63a4a44a8f63e52f314fa5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4d9d96fdab24132a727dcf107aa453a","max":826,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84f615f1d8a24460baa0f231b1b11b7f","value":826}},"fc30261990b942199521c8563b1ebe68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e1f3251a48e4e84bbf6bf9ddad747dd","placeholder":"​","style":"IPY_MODEL_a957db7936e34419beb250e775194ca5","value":" 826/826 [00:00&lt;00:00, 8176.44 examples/s]"}},"75b5f24372054091bf57b8c0ffa7f8f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad94266d3c424cfe9385f49f6d9930af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c291e0e834f0495eb3b258df2bb67a28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4d9d96fdab24132a727dcf107aa453a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84f615f1d8a24460baa0f231b1b11b7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e1f3251a48e4e84bbf6bf9ddad747dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a957db7936e34419beb250e775194ca5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"daa5bc7a3e604e8091c47f381a69121b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56e9ffb4b37c4a368f6f366c757226da","IPY_MODEL_c351d5c5830c421c9b7714e63a4cbcb6","IPY_MODEL_771b9da7079a4f57948f038a95654518"],"layout":"IPY_MODEL_6c8c8bdd44c741969a564ec11ad5c47c"}},"56e9ffb4b37c4a368f6f366c757226da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d692428d52be4b68a340becccc787766","placeholder":"​","style":"IPY_MODEL_22093d7484fe4041ad65ee7afec6f62d","value":"Map: 100%"}},"c351d5c5830c421c9b7714e63a4cbcb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5edacd54186e4a47b5a105dcb39dffac","max":207,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8ff8a203cd9449bb711324cb3104f92","value":207}},"771b9da7079a4f57948f038a95654518":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f469517e9fea42c2897a3cc630fe449c","placeholder":"​","style":"IPY_MODEL_c9fd4f92cf8349aba18187b22a549921","value":" 207/207 [00:00&lt;00:00, 5455.84 examples/s]"}},"6c8c8bdd44c741969a564ec11ad5c47c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d692428d52be4b68a340becccc787766":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22093d7484fe4041ad65ee7afec6f62d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5edacd54186e4a47b5a105dcb39dffac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8ff8a203cd9449bb711324cb3104f92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f469517e9fea42c2897a3cc630fe449c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9fd4f92cf8349aba18187b22a549921":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install -U transformers==4.43.3 accelerate==0.33.0 peft==0.11.1 trl==0.9.6 datasets==2.20.0 pandas==2.2.2\n","!pip install -U bitsandbytes==0.43.1\n","!pip install -U triton==2.3.0\n"],"metadata":{"id":"Pmrugs_m3TM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Klue -Roberta - small"],"metadata":{"id":"_wmA5PIRL3Mn"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import os, random\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding,\n","    set_seed,\n",")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ===============================\n","# 0) 시드 고정\n","# ===============================\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","set_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[INFO] device: {device}\")\n","\n","# ===============================\n","# 1) 데이터 로딩\n","# ===============================\n","CSV_PATH = r\"/content/drive/MyDrive/KDH/dataset/llm_datasets_0817_non_utf8.csv\"\n","df = pd.read_csv(CSV_PATH)\n","\n","df = df.dropna(subset=[\"label\", \"text\"])\n","df = df[df[\"label\"].isin([0, 1])]\n","df = df.reset_index(drop=True)\n","df[\"label\"] = df[\"label\"].astype(int)\n","df[\"text\"]  = df[\"text\"].astype(str)\n","\n","print(\"[INFO] Dataset size:\", len(df))\n","print(df[\"label\"].value_counts())\n","\n","# ===============================\n","# 2) 데이터 분할\n","# ===============================\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df[\"text\"], df[\"label\"],\n","    test_size=0.2,\n","    stratify=df[\"label\"],\n","    random_state=SEED\n",")\n","\n","# ===============================\n","# 3) Tokenizer & Dataset\n","# ===============================\n","MODEL_NAME = \"klue/roberta-small\"   # ✅ Roberta-small 사용\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.model_max_length = 512\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True)\n","\n","train_ds = Dataset.from_dict({\"text\": X_train.tolist(), \"labels\": y_train.tolist()})\n","test_ds  = Dataset.from_dict({\"text\": X_test.tolist(),  \"labels\": y_test.tolist()})\n","\n","train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","test_ds  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","\n","train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ===============================\n","# 4) 모델 로딩\n","# ===============================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME, num_labels=2\n",").to(device)\n","\n","# ===============================\n","# 5) 학습 설정\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./roberta-small-classifier\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    logging_steps=50,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    save_total_limit=2,\n","    fp16=torch.cuda.is_available(),\n","    warmup_ratio=0.1,\n","    report_to=\"none\"\n",")\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = logits.argmax(axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# ===============================\n","# 6) 학습\n","# ===============================\n","trainer.train()\n","\n","# ===============================\n","# 7) 예측 후 Confusion Matrix 출력\n","# ===============================\n","out = trainer.predict(test_ds)\n","logits = out.predictions\n","probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n","y_true = out.label_ids\n","y_pred = probs.argmax(axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"=== Confusion Matrix ===\")\n","print(cm)\n","\n","# ============================\n","# 추론 속도 + 파라미터 수 + 메모리 사용량\n","# -*- coding: utf-8 -*-\n","import os, random, time, psutil\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding,\n","    set_seed,\n",")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ===============================\n","# 0) 시드 고정\n","# ===============================\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","set_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[INFO] device: {device}\")\n","\n","# ===============================\n","# 1) 데이터 로딩\n","# ===============================\n","CSV_PATH = r\"/content/drive/MyDrive/KDH/dataset/llm_datasets_0817_non_utf8.csv\"\n","df = pd.read_csv(CSV_PATH)\n","\n","df = df.dropna(subset=[\"label\", \"text\"])\n","df = df[df[\"label\"].isin([0, 1])]\n","df = df.reset_index(drop=True)\n","df[\"label\"] = df[\"label\"].astype(int)\n","df[\"text\"]  = df[\"text\"].astype(str)\n","\n","print(\"[INFO] Dataset size:\", len(df))\n","print(df[\"label\"].value_counts())\n","\n","# ===============================\n","# 2) 데이터 분할\n","# ===============================\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df[\"text\"], df[\"label\"],\n","    test_size=0.2,\n","    stratify=df[\"label\"],\n","    random_state=SEED\n",")\n","\n","# ===============================\n","# 3) Tokenizer & Dataset\n","# ===============================\n","MODEL_NAME = \"klue/roberta-small\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.model_max_length = 512\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True)\n","\n","train_ds = Dataset.from_dict({\"text\": X_train.tolist(), \"labels\": y_train.tolist()})\n","test_ds  = Dataset.from_dict({\"text\": X_test.tolist(),  \"labels\": y_test.tolist()})\n","\n","train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","test_ds  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","\n","train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ===============================\n","# 4) 모델 로딩\n","# ===============================\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    MODEL_NAME, num_labels=2\n",").to(device)\n","\n","# ===============================\n","# 5) 학습 설정\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./roberta-small-classifier\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    logging_steps=50,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    save_total_limit=2,\n","    fp16=torch.cuda.is_available(),\n","    warmup_ratio=0.1,\n","    report_to=\"none\"\n",")\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = logits.argmax(axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# ===============================\n","# 6) 학습\n","# ===============================\n","trainer.train()\n","\n","# ===============================\n","# 7) 예측 후 Confusion Matrix 출력\n","# ===============================\n","out = trainer.predict(test_ds)\n","logits = out.predictions\n","probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n","y_true = out.label_ids\n","y_pred = probs.argmax(axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"=== Confusion Matrix ===\")\n","print(cm)\n","\n","# ===============================\n","# 8) 추가 성능/효율 지표\n","# ===============================\n","# ===============================\n","# 8) 추가 성능/효율 지표  (교체본)\n","# ===============================\n","from torch.utils.data import DataLoader\n","\n","# (1) 추론 속도 측정 — DataLoader + data_collator 사용(패딩/타입 안전)\n","eval_loader = DataLoader(test_ds, batch_size=32, collate_fn=data_collator)\n","\n","batch = next(iter(eval_loader))              # {'input_ids': [B,L], 'attention_mask': [B,L], 'labels': [B]}\n","batch.pop(\"labels\", None)                    # 라벨 제거\n","batch = {k: v.to(device) for k, v in batch.items()}  # 디바이스 이동\n","\n","model.eval()\n","with torch.no_grad():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.reset_peak_memory_stats()  # ⬅️ 추론 피크메모리만 보려면 리셋\n","    t0 = time.time()\n","    _ = model(**batch)\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t1 = time.time()\n","\n","B = batch[\"input_ids\"].size(0)\n","latency = (t1 - t0) / B\n","throughput = B / (t1 - t0)\n","\n","print(f\"\\n=== 추론 속도 ===\")\n","print(f\"평균 Latency: {latency*1000:.2f} ms/샘플\")\n","print(f\"Throughput : {throughput:.2f} 샘플/초\")\n","\n","# (2) 파라미터 수\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"\\n=== 파라미터 수 ===\")\n","print(f\"전체 파라미터 수: {total_params/1e6:.2f} M\")\n","print(f\"학습가능 파라미터 수: {trainable_params/1e6:.2f} M\")\n","\n","# (3) 메모리 사용량\n","print(f\"\\n=== 메모리 사용량 ===\")\n","if torch.cuda.is_available():\n","    allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n","    reserved  = torch.cuda.max_memory_reserved(device)  / (1024**2)\n","    print(f\"GPU 메모리 (할당): {allocated:.2f} MB\")\n","    print(f\"GPU 메모리 (예약): {reserved:.2f} MB\")\n","else:\n","    print(f\"CPU 메모리 사용량: {psutil.Process(os.getpid()).memory_info().rss/(1024**2):.2f} MB\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["def74396aa0a4e3d912a8c253d6da3b8","f540fa8769444b2c87cd9ec620071c59","06c163b0ec4a4cd29df2860f4deec9f5","d9ce2819d65343d0b5e5d022b20b2fe7","3bb468991c91487da3dc8bee84f70fb9","3621182a577e45babe2f829a17d0a34d","b044c67c071b4e3fb4b65839ca3e8b03","1f826b111a79435fbb9ec4cab7122d6e","3179180f665c4ec9bb58d1d6bb34ed8e","04dfe4506fb745cfa08fc4b9acd1b00e","0400c32a85d74668a38ebcaec1b8e3ab","13b459545f6942af8b60f58df417772b","2d1731709ef942d79a2e63ba676b96f9","85a2478632dc4f7d84ea5f13bb4baef1","27e3e8477f904877831ec43e2c9b1b1a","ec9f760134834368a07b73e545fbd2e2","19a37996aa584ba6a7b130346538de7c","8c72e2af49184f0bb227697b78b59e01","5639c505f99f4384a15e1f1522c6651b","6077771553d847dfb3cf6d139539a0fd","ebba6a4384c541099f1c30bb1d2392e2","94dc8636a5a644dba9a6fb043f3524bb","dea98ec2fe9a44cc875a56beb134b974","1037670973bb4151acb6c815cc28cb30","4eacf772aba843c79cf5f219f530b3bd","a3c0d7637bd04c4fb761c56c4ae5b831","1875d5a28e1e43ebb341f12873bcb9a9","3e17d9029e0b4bad8e7f283ffc3fdb39","ef681725a30b4d81a7b2ea896ec5b562","d0f29a30fc644cb49a6b79d35989b900","277ca93362024a869a15abf1b8b95ac7","d0f18e51a1b844d9bc700bbeef8df473","ec643b12e18c4cb6bcb189cb62615db0","ca096f4ccf4b4513a9af6c034d6a9a01","0b39d494cdf949afad772ff205d25630","1f6624b16a064e19bed6c9110e0d94ef","8f694a73b8d4474eb2230a9962843575","94f5daeb30994167b817b3e54d8b1d44","6a88c76c762e42caa9235f20b151c75c","59c567cf1473453ba86d4421ef7469b4","e8d62948a5594d7f943bdee821491f44","b0a986536baf406b8d11f587b3bdac45","d6d06baecf19481291fd4c7b7609d245","704ff93516de4842a3fa959656ebe2db"]},"id":"msq3JnrQL1c2","executionInfo":{"status":"ok","timestamp":1755784447941,"user_tz":-540,"elapsed":65032,"user":{"displayName":"jiwon song","userId":"09013595362968296143"}},"outputId":"b97b9c19-a4b4-4542-f0b1-895464480ca9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[INFO] device: cuda\n","[INFO] Dataset size: 1033\n","label\n","1    702\n","0    331\n","Name: count, dtype: int64\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/826 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def74396aa0a4e3d912a8c253d6da3b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/207 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b459545f6942af8b60f58df417772b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [520/520 00:27, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.235300</td>\n","      <td>0.182160</td>\n","      <td>0.917874</td>\n","      <td>0.939502</td>\n","      <td>0.942857</td>\n","      <td>0.936170</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.235700</td>\n","      <td>0.250313</td>\n","      <td>0.937198</td>\n","      <td>0.953737</td>\n","      <td>0.957143</td>\n","      <td>0.950355</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.089800</td>\n","      <td>0.149990</td>\n","      <td>0.961353</td>\n","      <td>0.971831</td>\n","      <td>0.965035</td>\n","      <td>0.978723</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.015300</td>\n","      <td>0.299058</td>\n","      <td>0.937198</td>\n","      <td>0.953737</td>\n","      <td>0.957143</td>\n","      <td>0.950355</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000500</td>\n","      <td>0.308642</td>\n","      <td>0.942029</td>\n","      <td>0.957143</td>\n","      <td>0.964029</td>\n","      <td>0.950355</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== Confusion Matrix ===\n","[[ 61   5]\n"," [  3 138]]\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[INFO] device: cuda\n","[INFO] Dataset size: 1033\n","label\n","1    702\n","0    331\n","Name: count, dtype: int64\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/826 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dea98ec2fe9a44cc875a56beb134b974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/207 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca096f4ccf4b4513a9af6c034d6a9a01"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [520/520 00:30, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.235300</td>\n","      <td>0.182160</td>\n","      <td>0.917874</td>\n","      <td>0.939502</td>\n","      <td>0.942857</td>\n","      <td>0.936170</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.235700</td>\n","      <td>0.250313</td>\n","      <td>0.937198</td>\n","      <td>0.953737</td>\n","      <td>0.957143</td>\n","      <td>0.950355</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.089800</td>\n","      <td>0.149990</td>\n","      <td>0.961353</td>\n","      <td>0.971831</td>\n","      <td>0.965035</td>\n","      <td>0.978723</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.015300</td>\n","      <td>0.299058</td>\n","      <td>0.937198</td>\n","      <td>0.953737</td>\n","      <td>0.957143</td>\n","      <td>0.950355</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000500</td>\n","      <td>0.308642</td>\n","      <td>0.942029</td>\n","      <td>0.957143</td>\n","      <td>0.964029</td>\n","      <td>0.950355</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== Confusion Matrix ===\n","[[ 61   5]\n"," [  3 138]]\n","\n","=== 추론 속도 ===\n","평균 Latency: 0.40 ms/샘플\n","Throughput : 2499.26 샘플/초\n","\n","=== 파라미터 수 ===\n","전체 파라미터 수: 68.09 M\n","학습가능 파라미터 수: 68.09 M\n","\n","=== 메모리 사용량 ===\n","GPU 메모리 (할당): 1152.30 MB\n","GPU 메모리 (예약): 1874.00 MB\n"]}]},{"cell_type":"markdown","source":["# Klue-Roberta-small + base - Knowledge Distillation"],"metadata":{"id":"4N9odA8zNyLP"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import os, random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    classification_report,\n",")\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding,\n","    set_seed,\n",")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ===============================\n","# 0) 시드 고정\n","# ===============================\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[INFO] device: {device}\")\n","\n","# ===============================\n","# 1) 데이터 로딩\n","# ===============================\n","CSV_PATH = r\"/content/drive/MyDrive/KDH/dataset/llm_datasets_0817_non_utf8.csv\"\n","df = pd.read_csv(CSV_PATH)\n","\n","df = df.dropna(subset=[\"label\", \"text\"]).copy()\n","df = df[df[\"label\"].isin([0, 1])].copy()\n","df[\"label\"] = df[\"label\"].astype(np.int64)   # torch.long\n","\n","df[\"text\"]  = df[\"text\"].astype(str)\n","df = df.reset_index(drop=True)\n","\n","print(\"[INFO] unique labels:\", sorted(df[\"label\"].unique()))\n","\n","# ===============================\n","# 2) 데이터 분할\n","# ===============================\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df[\"text\"], df[\"label\"],\n","    test_size=0.2,\n","    stratify=df[\"label\"],\n","    random_state=SEED\n",")\n","\n","train_ds = Dataset.from_dict({\n","    \"text\": X_train.tolist(),\n","    \"labels\": y_train.astype(np.int64).tolist()\n","})\n","\n","test_ds  = Dataset.from_dict({\"text\": X_test.tolist(),\n","                              \"labels\": y_test.astype(np.int64).tolist()})\n","\n","# ===============================\n","# 3) Tokenizer (학생 기준)\n","# ===============================\n","teacher_model_name = \"klue/roberta-base\"\n","student_model_name = \"klue/roberta-small\"\n","\n","# ✅ Roberta tokenizer 하나만 사용\n","tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","train_enc = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","test_enc  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","\n","train_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","# ✅ collator도 같은 tokenizer\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","# ===============================\n","# 4) 모델 로딩\n","# ===============================\n","teacher = AutoModelForSequenceClassification.from_pretrained(\n","    teacher_model_name, num_labels=2\n",").to(device)\n","\n","student = AutoModelForSequenceClassification.from_pretrained(\n","    student_model_name, num_labels=2\n",").to(device)\n","\n","teacher.eval()  # 교사는 고정(freeze)\n","\n","# ===============================\n","# 5) Distillation Trainer 정의\n","# ===============================\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.teacher = teacher_model.eval()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs[\"labels\"].view(-1).to(torch.long)\n","\n","\n","        # 라벨 shape/dtype 보정\n","        labels = labels.view(-1).to(torch.long)\n","\n","        student_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","        outputs_student = model(**student_inputs)\n","        student_logits = (\n","            outputs_student.logits if hasattr(outputs_student, \"logits\") else outputs_student[0]\n","        )\n","        loss_ce = self.ce_loss(student_logits, labels)\n","\n","        # 👉 교사 입력 만들기\n","        teacher_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","        with torch.no_grad():\n","            outputs_teacher = self.teacher(**teacher_inputs)\n","            teacher_logits = (\n","                outputs_teacher.logits if hasattr(outputs_teacher, \"logits\") else outputs_teacher[0]\n","            )\n","\n","\n","        # NaN/Inf guard\n","        if torch.isnan(student_logits).any() or torch.isnan(teacher_logits).any():\n","            raise FloatingPointError(\"NaN detected in logits\")\n","\n","        # KD loss\n","        T = self.temperature\n","        loss_kd = F.kl_div(\n","            F.log_softmax(student_logits / T, dim=-1),\n","            F.softmax(teacher_logits / T, dim=-1),\n","            reduction=\"batchmean\"\n","        ) * (T * T)\n","\n","        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n","        return (loss, {\"logits\": student_logits}) if return_outputs else loss\n","\n","\n","\n","\n","# ===============================\n","# 6) 학습 설정\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./distilled-klue/roberta-small\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    save_total_limit=2,\n","    fp16=False,              # Colab에서 안정성 위해 끔\n","    warmup_ratio=0.1,\n","    report_to=\"none\",\n","    save_safetensors=False   # ✅ contiguous 에러 방지\n",")\n","\n","# ===============================\n","# 7) 평가지표\n","# ===============================\n","def compute_metrics(eval_pred):\n","    logits = eval_pred.predictions\n","    labels = eval_pred.label_ids\n","\n","    # ✅ predictions가 tuple일 때 처리\n","    if isinstance(logits, tuple):\n","        if len(logits) > 0:\n","            logits = logits[0]\n","        else:\n","            raise ValueError(\"logits is an empty tuple!\")\n","\n","    preds = logits.argmax(axis=-1)\n","    p, r, f1, _ = precision_recall_fscore_support(\n","        labels, preds, average=\"binary\", pos_label=1\n","    )\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n","\n","\n","\n","\n","# ===============================\n","# 8) Trainer 실행\n","# ===============================\n","trainer = DistillationTrainer(\n","    model=student,\n","    teacher_model=teacher,\n","    args=training_args,\n","    train_dataset=train_enc,\n","    eval_dataset=test_enc,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","# ===============================\n","# 9) Confusion Matrix 출력\n","# ===============================\n","out = trainer.predict(test_enc)\n","logits = out.predictions\n","probs = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n","y_true = out.label_ids\n","y_pred = probs.argmax(axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"=== Confusion Matrix ===\")\n","print(cm)\n","\n","print(\"=== Classification Report ===\")\n","print(classification_report(y_true, y_pred, digits=4))\n","\n","# ===============================\n","# 10) 추가 성능/효율 지표  (수정본)\n","# ===============================\n","import time, psutil\n","from torch.utils.data import DataLoader\n","\n","# ✅ 반드시 토크나이즈된 평가셋 사용\n","eval_loader = DataLoader(test_enc, batch_size=32, collate_fn=data_collator)\n","\n","# 배치 하나 가져오기\n","batch = next(iter(eval_loader))  # dict_keys(['input_ids','attention_mask','labels'])\n","# 라벨 제거 + 디바이스 이동\n","batch_no_label = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n","\n","# ✅ 변수명 일치: student 모델로 추론\n","student.eval()\n","with torch.no_grad():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.reset_peak_memory_stats()  # 피크 메모리 리셋\n","    t0 = time.time()\n","    _ = student(**batch_no_label)\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t1 = time.time()\n","\n","B = batch_no_label[\"input_ids\"].size(0)\n","latency = (t1 - t0) / B\n","throughput = B / (t1 - t0)\n","\n","print(f\"\\n=== 추론 속도 ===\")\n","print(f\"평균 Latency: {latency*1000:.2f} ms/샘플\")\n","print(f\"Throughput : {throughput:.2f} 샘플/초\")\n","\n","# (2) 파라미터 수 — student 기준\n","total_params = sum(p.numel() for p in student.parameters())\n","trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n","print(f\"\\n=== 파라미터 수 ===\")\n","print(f\"전체 파라미터 수: {total_params/1e6:.2f} M\")\n","print(f\"학습가능 파라미터 수: {trainable_params/1e6:.2f} M\")\n","\n","# (3) 메모리 사용량\n","print(f\"\\n=== 메모리 사용량 ===\")\n","if torch.cuda.is_available():\n","    allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n","    reserved  = torch.cuda.max_memory_reserved(device)  / (1024**2)\n","    print(f\"GPU 메모리 (할당): {allocated:.2f} MB\")\n","    print(f\"GPU 메모리 (예약): {reserved:.2f} MB\")\n","else:\n","    print(f\"CPU 메모리 사용량: {psutil.Process(os.getpid()).memory_info().rss/(1024**2):.2f} MB\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":944,"referenced_widgets":["b68701ea3f5346ef8e348f05a6b900fb","c1364764c3b64969a11b0c9b967405c8","90821a3f257f4bf1ba54516fe88a7fee","267aace7482642e59b5099f2585cd155","c4f6b511e43a427cbd8e97d920e61b7c","a084d62cfd13429091f4b842006dc560","382d584e46f9483bafef8ec03b970875","adb21e1877a94c61841ddfb13c0d6802","a058aae6005f413b820a8489e39b7fbd","99139421b0cc454cb58c13186e2fcc15","dda3cf3bb8d244dbbf9a731399b130db","226a1b69731d44d882928ed2cf2afa64","a58ebfe50e1440829bf5759d71187596","e7d44da5c6b845eda98222532344fbfd","8db4f07e12d24e99b2d4af48f2f8f4c7","82b7801844d64cf9b982dca90d0f81a0","f8b3680e4aa84219af0190f3e27d0d38","97c1900fad534530a520bfe18e1f7645","ea98d33bf0fa4e06827f55fefe4b5959","addbd6f6ebab4810a95061c9fce4cc72","c490b7bbf85841679ec21acbb830de99","93aaac1326084f14938a1918c474b1c0"]},"id":"YbOAKlWFOMyy","executionInfo":{"status":"ok","timestamp":1755784955959,"user_tz":-540,"elapsed":31470,"user":{"displayName":"jiwon song","userId":"09013595362968296143"}},"outputId":"f7e6f2ba-b06e-4968-c40a-3af4d50ac790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[INFO] device: cuda\n","[INFO] unique labels: [0, 1]\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/826 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68701ea3f5346ef8e348f05a6b900fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/207 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"226a1b69731d44d882928ed2cf2afa64"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [260/260 00:27, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.245622</td>\n","      <td>0.927536</td>\n","      <td>0.925676</td>\n","      <td>0.971631</td>\n","      <td>0.948097</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.233949</td>\n","      <td>0.937198</td>\n","      <td>0.950704</td>\n","      <td>0.957447</td>\n","      <td>0.954064</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.230061</td>\n","      <td>0.961353</td>\n","      <td>0.958621</td>\n","      <td>0.985816</td>\n","      <td>0.972028</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.234607</td>\n","      <td>0.932367</td>\n","      <td>0.937931</td>\n","      <td>0.964539</td>\n","      <td>0.951049</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>0.233800</td>\n","      <td>0.946860</td>\n","      <td>0.985075</td>\n","      <td>0.936170</td>\n","      <td>0.960000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== Confusion Matrix ===\n","[[ 60   6]\n"," [  2 139]]\n","=== Classification Report ===\n","              precision    recall  f1-score   support\n","\n","           0     0.9677    0.9091    0.9375        66\n","           1     0.9586    0.9858    0.9720       141\n","\n","    accuracy                         0.9614       207\n","   macro avg     0.9632    0.9475    0.9548       207\n","weighted avg     0.9615    0.9614    0.9610       207\n","\n","\n","=== 추론 속도 ===\n","평균 Latency: 0.28 ms/샘플\n","Throughput : 3578.28 샘플/초\n","\n","=== 파라미터 수 ===\n","전체 파라미터 수: 68.09 M\n","학습가능 파라미터 수: 68.09 M\n","\n","=== 메모리 사용량 ===\n","GPU 메모리 (할당): 1698.07 MB\n","GPU 메모리 (예약): 2400.00 MB\n"]}]},{"cell_type":"markdown","source":["# torch Pruning"],"metadata":{"id":"WsYl4XY9UI2_"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import os, random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.utils.prune as prune\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    classification_report,\n",")\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding,\n","    set_seed,\n",")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ===============================\n","# 0) 시드 고정\n","# ===============================\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[INFO] device: {device}\")\n","\n","# ===============================\n","# 1) 데이터 로딩\n","# ===============================\n","CSV_PATH = r\"/content/drive/MyDrive/KDH/dataset/llm_datasets_0817_non_utf8.csv\"\n","df = pd.read_csv(CSV_PATH)\n","\n","df = df.dropna(subset=[\"label\", \"text\"]).copy()\n","df = df[df[\"label\"].isin([0, 1])].copy()\n","df[\"label\"] = df[\"label\"].astype(np.int64)   # torch.long\n","df[\"text\"]  = df[\"text\"].astype(str)\n","df = df.reset_index(drop=True)\n","\n","print(\"[INFO] unique labels:\", sorted(df[\"label\"].unique()))\n","\n","# ===============================\n","# 2) 데이터 분할\n","# ===============================\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df[\"text\"], df[\"label\"],\n","    test_size=0.2,\n","    stratify=df[\"label\"],\n","    random_state=SEED\n",")\n","\n","train_ds = Dataset.from_dict({\n","    \"text\": X_train.tolist(),\n","    \"labels\": y_train.astype(np.int64).tolist()\n","})\n","\n","test_ds  = Dataset.from_dict({\n","    \"text\": X_test.tolist(),\n","    \"labels\": y_test.astype(np.int64).tolist()\n","})\n","\n","# ===============================\n","# 3) Tokenizer\n","# ===============================\n","teacher_model_name = \"klue/roberta-base\"\n","student_model_name = \"klue/roberta-small\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","train_enc = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","test_enc  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","\n","train_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ===============================\n","# 4) 모델 로딩\n","# ===============================\n","teacher = AutoModelForSequenceClassification.from_pretrained(\n","    teacher_model_name, num_labels=2\n",").to(device)\n","\n","student = AutoModelForSequenceClassification.from_pretrained(\n","    student_model_name, num_labels=2\n",").to(device)\n","\n","teacher.eval()\n","\n","# ===============================\n","# 5) Distillation Trainer\n","# ===============================\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.teacher = teacher_model.eval()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs[\"labels\"].view(-1).to(torch.long)\n","\n","        student_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","        outputs_student = model(**student_inputs)\n","\n","        # ✅ logits 안전 처리\n","        student_logits = outputs_student.logits if hasattr(outputs_student, \"logits\") else outputs_student[0]\n","\n","        loss_ce = self.ce_loss(student_logits, labels)\n","\n","        # teacher logits\n","        with torch.no_grad():\n","            outputs_teacher = self.teacher(**student_inputs)\n","            teacher_logits = outputs_teacher.logits if hasattr(outputs_teacher, \"logits\") else outputs_teacher[0]\n","\n","        # KD loss\n","        T = self.temperature\n","        loss_kd = F.kl_div(\n","            F.log_softmax(student_logits / T, dim=-1),\n","            F.softmax(teacher_logits / T, dim=-1),\n","            reduction=\"batchmean\"\n","        ) * (T * T)\n","\n","        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n","        return (loss, {\"logits\": student_logits}) if return_outputs else loss\n","\n","# ===============================\n","# 6) 학습 설정\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./distilled-klue/roberta-small\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    save_total_limit=2,\n","    fp16=False,\n","    warmup_ratio=0.1,\n","    report_to=\"none\",\n","    save_safetensors=False\n",")\n","\n","# ===============================\n","# 7) 평가지표\n","# ===============================\n","def compute_metrics(eval_pred):\n","    logits = eval_pred.predictions\n","    labels = eval_pred.label_ids\n","\n","    if isinstance(logits, tuple):\n","        logits = logits[0]\n","\n","    preds = logits.argmax(axis=-1)\n","    p, r, f1, _ = precision_recall_fscore_support(\n","        labels, preds, average=\"binary\", pos_label=1\n","    )\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n","\n","# ===============================\n","# 8) KD 학습\n","# ===============================\n","trainer = DistillationTrainer(\n","    model=student,\n","    teacher_model=teacher,\n","    args=training_args,\n","    train_dataset=train_enc,\n","    eval_dataset=test_enc,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","# ===============================\n","# 9) Pruning 적용\n","# ===============================\n","print(\"\\n[INFO] Applying pruning ...\")\n","\n","for name, module in student.named_modules():\n","    if isinstance(module, torch.nn.Linear):\n","        prune.l1_unstructured(module, name=\"weight\", amount=0.5)\n","\n","# Sparsity 확인\n","for name, module in student.named_modules():\n","    if isinstance(module, torch.nn.Linear) and hasattr(module, \"weight_mask\"):\n","        sparsity = 100. * float(torch.sum(module.weight == 0)) / module.weight.nelement()\n","        print(f\"Layer {name} | Sparsity: {sparsity:.2f}%\")\n","\n","# (선택) mask 병합\n","for name, module in student.named_modules():\n","    if isinstance(module, torch.nn.Linear):\n","        prune.remove(module, \"weight\")\n","\n","# ===============================\n","# 10) 평가\n","# ===============================\n","out = trainer.predict(test_enc)\n","logits = out.predictions\n","probs = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n","y_true = out.label_ids\n","y_pred = probs.argmax(axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"=== Confusion Matrix ===\")\n","print(cm)\n","\n","print(\"=== Classification Report ===\")\n","print(classification_report(y_true, y_pred, digits=4))\n","\n","# ===============================\n","# 11) 저장\n","# ===============================\n","student.save_pretrained(\"./distilled-roberta-pruned\")\n","tokenizer.save_pretrained(\"./distilled-roberta-pruned\")\n","\n","# 10) 추가 성능/효율 지표  (수정본)\n","# ===============================\n","import time, psutil\n","from torch.utils.data import DataLoader\n","\n","# ✅ 반드시 토크나이즈된 평가셋 사용\n","eval_loader = DataLoader(test_enc, batch_size=32, collate_fn=data_collator)\n","\n","# 배치 하나 가져오기\n","batch = next(iter(eval_loader))  # dict_keys(['input_ids','attention_mask','labels'])\n","# 라벨 제거 + 디바이스 이동\n","batch_no_label = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n","\n","# ✅ 변수명 일치: student 모델로 추론\n","student.eval()\n","with torch.no_grad():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.reset_peak_memory_stats()  # 피크 메모리 리셋\n","    t0 = time.time()\n","    _ = student(**batch_no_label)\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t1 = time.time()\n","\n","B = batch_no_label[\"input_ids\"].size(0)\n","latency = (t1 - t0) / B\n","throughput = B / (t1 - t0)\n","\n","print(f\"\\n=== 추론 속도 ===\")\n","print(f\"평균 Latency: {latency*1000:.2f} ms/샘플\")\n","print(f\"Throughput : {throughput:.2f} 샘플/초\")\n","\n","# (2) 파라미터 수 — student 기준\n","total_params = sum(p.numel() for p in student.parameters())\n","trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n","print(f\"\\n=== 파라미터 수 ===\")\n","print(f\"전체 파라미터 수: {total_params/1e6:.2f} M\")\n","print(f\"학습가능 파라미터 수: {trainable_params/1e6:.2f} M\")\n","\n","# (3) 메모리 사용량\n","print(f\"\\n=== 메모리 사용량 ===\")\n","if torch.cuda.is_available():\n","    allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n","    reserved  = torch.cuda.max_memory_reserved(device)  / (1024**2)\n","    print(f\"GPU 메모리 (할당): {allocated:.2f} MB\")\n","    print(f\"GPU 메모리 (예약): {reserved:.2f} MB\")\n","else:\n","    print(f\"CPU 메모리 사용량: {psutil.Process(os.getpid()).memory_info().rss/(1024**2):.2f} MB\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6681465823a34b09be2dbeb4df5eebc6","7fd8db3e9c7e4407baf3405ace0e4f3f","7259009ca63a4a44a8f63e52f314fa5b","fc30261990b942199521c8563b1ebe68","75b5f24372054091bf57b8c0ffa7f8f2","ad94266d3c424cfe9385f49f6d9930af","c291e0e834f0495eb3b258df2bb67a28","b4d9d96fdab24132a727dcf107aa453a","84f615f1d8a24460baa0f231b1b11b7f","3e1f3251a48e4e84bbf6bf9ddad747dd","a957db7936e34419beb250e775194ca5","daa5bc7a3e604e8091c47f381a69121b","56e9ffb4b37c4a368f6f366c757226da","c351d5c5830c421c9b7714e63a4cbcb6","771b9da7079a4f57948f038a95654518","6c8c8bdd44c741969a564ec11ad5c47c","d692428d52be4b68a340becccc787766","22093d7484fe4041ad65ee7afec6f62d","5edacd54186e4a47b5a105dcb39dffac","b8ff8a203cd9449bb711324cb3104f92","f469517e9fea42c2897a3cc630fe449c","c9fd4f92cf8349aba18187b22a549921"]},"id":"clKeDY5FZiHc","executionInfo":{"status":"ok","timestamp":1755785281240,"user_tz":-540,"elapsed":20925,"user":{"displayName":"jiwon song","userId":"09013595362968296143"}},"outputId":"a9a8c074-ef0c-46e2-ce2d-11b2481110a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[INFO] device: cuda\n","[INFO] unique labels: [0, 1]\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/826 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6681465823a34b09be2dbeb4df5eebc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/207 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa5bc7a3e604e8091c47f381a69121b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [156/156 00:15, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.239210</td>\n","      <td>0.951691</td>\n","      <td>0.939597</td>\n","      <td>0.992908</td>\n","      <td>0.965517</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.229788</td>\n","      <td>0.956522</td>\n","      <td>0.945946</td>\n","      <td>0.992908</td>\n","      <td>0.968858</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.229716</td>\n","      <td>0.961353</td>\n","      <td>0.971631</td>\n","      <td>0.971631</td>\n","      <td>0.971631</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","[INFO] Applying pruning ...\n","Layer roberta.encoder.layer.0.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.0.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.0.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.0.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.0.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.0.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.1.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.2.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.3.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.4.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.attention.self.query | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.attention.self.key | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.attention.self.value | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.attention.output.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.intermediate.dense | Sparsity: 50.00%\n","Layer roberta.encoder.layer.5.output.dense | Sparsity: 50.00%\n","Layer classifier.dense | Sparsity: 50.00%\n","Layer classifier.out_proj | Sparsity: 50.00%\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=== Confusion Matrix ===\n","[[ 62   4]\n"," [  7 134]]\n","=== Classification Report ===\n","              precision    recall  f1-score   support\n","\n","           0     0.8986    0.9394    0.9185        66\n","           1     0.9710    0.9504    0.9606       141\n","\n","    accuracy                         0.9469       207\n","   macro avg     0.9348    0.9449    0.9395       207\n","weighted avg     0.9479    0.9469    0.9472       207\n","\n","\n","=== 추론 속도 ===\n","평균 Latency: 0.32 ms/샘플\n","Throughput : 3167.08 샘플/초\n","\n","=== 파라미터 수 ===\n","전체 파라미터 수: 68.09 M\n","학습가능 파라미터 수: 68.09 M\n","\n","=== 메모리 사용량 ===\n","GPU 메모리 (할당): 1866.37 MB\n","GPU 메모리 (예약): 2422.00 MB\n"]}]},{"cell_type":"markdown","source":["# QLoRA + KD"],"metadata":{"id":"XIo8rFdbacjo"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","import os, random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    classification_report,\n",")\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModel,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorWithPadding,\n","    set_seed,\n","    BitsAndBytesConfig\n",")\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","\n","# ===============================\n","# 0) 시드 고정\n","# ===============================\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[INFO] device: {device}\")\n","\n","# ===============================\n","# 1) 데이터 로딩\n","# ===============================\n","CSV_PATH = r\"/home/piai/PoscoAIproject/dataset/llm_datasets_0817_non_utf8.csv\"\n","df = pd.read_csv(CSV_PATH)\n","\n","df = df.dropna(subset=[\"label\", \"text\"]).copy()\n","df = df[df[\"label\"].isin([0, 1])].copy()\n","df[\"label\"] = df[\"label\"].astype(np.int64)\n","df[\"text\"]  = df[\"text\"].astype(str)\n","df = df.reset_index(drop=True)\n","print(\"[INFO] unique labels:\", sorted(df[\"label\"].unique()))\n","\n","# ===============================\n","# 2) 데이터 분할\n","# ===============================\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df[\"text\"], df[\"label\"],\n","    test_size=0.2,\n","    stratify=df[\"label\"],\n","    random_state=SEED\n",")\n","\n","train_ds = Dataset.from_dict({\n","    \"text\": X_train.tolist(),\n","    \"labels\": y_train.astype(np.int64).tolist()\n","})\n","test_ds  = Dataset.from_dict({\n","    \"text\": X_test.tolist(),\n","    \"labels\": y_test.astype(np.int64).tolist()\n","})\n","\n","# ===============================\n","# 3) Tokenizer\n","# ===============================\n","teacher_model_name = \"klue/roberta-base\"\n","student_model_name = \"klue/roberta-small\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n","\n","def tokenize_fn(batch):\n","    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","train_enc = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","test_enc  = test_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","\n","train_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_enc.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ===============================\n","# 4) 모델 로딩 (Teacher / Student QLoRA)\n","# ===============================\n","# Teacher (float32 유지)\n","teacher = AutoModelForSequenceClassification.from_pretrained(\n","    teacher_model_name, num_labels=2,\n","    torch_dtype=torch.float32,\n","    device_map=\"auto\"\n",")\n","teacher.eval()\n","\n","# Student (Base는 4bit, Head는 float)\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","# 1) Base encoder만 4bit로 로드\n","base_model = AutoModel.from_pretrained(\n","    student_model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\"\n",")\n","\n","# 2) Classifier head는 float로 로드\n","student = AutoModelForSequenceClassification.from_pretrained(\n","    student_model_name,\n","    quantization_config=None,   # classifier는 양자화 안 함\n","    num_labels=2,\n","    device_map=\"auto\"\n",")\n","\n","# 3) Base encoder 교체 + classifier는 float 보장\n","student.roberta = base_model\n","student.classifier = student.classifier.to(torch.float32)\n","\n","# --- 핵심: 항상 dict 출력 강제 ---\n","if hasattr(teacher, \"config\"): teacher.config.return_dict = True\n","if hasattr(student, \"config\"): student.config.return_dict = True\n","if hasattr(base_model, \"config\"): base_model.config.return_dict = True\n","\n","# 4) QLoRA 준비\n","student.gradient_checkpointing_enable()\n","student.config.use_cache = False\n","student = prepare_model_for_kbit_training(student)\n","\n","# 5) LoRA config\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # RoBERTa 구조 기준\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"SEQ_CLS\"\n",")\n","student = get_peft_model(student, lora_config)\n","\n","# ===============================\n","# 5) Distillation Trainer 정의\n","# ===============================\n","def _get_logits(outputs):\n","    # ModelOutput(dict-유사) 또는 tuple 모두 안전 처리\n","    if hasattr(outputs, \"logits\"):\n","        return outputs.logits\n","    if isinstance(outputs, (tuple, list)):\n","        return outputs[0]\n","    raise TypeError(\"Model outputs do not contain logits.\")\n","\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.teacher = teacher_model.eval()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs[\"labels\"].view(-1).to(torch.long)\n","\n","        # Student forward (dict 강제 + 안전 추출)\n","        student_inputs = {k: v for k, v in inputs.items() if k != \"labels\"}\n","        outputs_student = model(**student_inputs, return_dict=True)\n","        student_logits = _get_logits(outputs_student).to(torch.float32)\n","        loss_ce = self.ce_loss(student_logits, labels)\n","\n","        # Teacher forward (dict 강제 + 안전 추출)\n","        teacher_device = next(self.teacher.parameters()).device\n","        teacher_inputs = {k: v.to(teacher_device) for k, v in student_inputs.items()}\n","        with torch.no_grad():\n","            outputs_teacher = self.teacher(**teacher_inputs, return_dict=True)\n","            teacher_logits = _get_logits(outputs_teacher).to(torch.float32)\n","\n","        # KD loss\n","        T = self.temperature\n","        loss_kd = F.kl_div(\n","            F.log_softmax(student_logits / T, dim=-1),\n","            F.softmax(teacher_logits / T, dim=-1),\n","            reduction=\"batchmean\"\n","        ) * (T * T)\n","\n","        loss = self.alpha * loss_ce + (1 - self.alpha) * loss_kd\n","        return (loss, {\"logits\": student_logits}) if return_outputs else loss\n","\n","# ===============================\n","# 6) 학습 설정\n","# ===============================\n","training_args = TrainingArguments(\n","    output_dir=\"./qlora-distilled-roberta-small\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",  # <- 보통 'f1'로 둡니다\n","    greater_is_better=True,\n","    save_total_limit=2,\n","    fp16=True,\n","    gradient_checkpointing=True,\n","    warmup_ratio=0.1,\n","    report_to=\"none\",\n","    save_safetensors=False\n",")\n","\n","# ===============================\n","# 7) 평가지표\n","# ===============================\n","def compute_metrics(eval_pred):\n","    logits = eval_pred.predictions\n","    labels = eval_pred.label_ids\n","    if isinstance(logits, (tuple, list)):\n","        logits = logits[0]\n","    preds = logits.argmax(axis=-1)\n","    p, r, f1, _ = precision_recall_fscore_support(\n","        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n","    )\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n","\n","# ===============================\n","# 8) Trainer 실행\n","# ===============================\n","trainer = DistillationTrainer(\n","    model=student,\n","    teacher_model=teacher,\n","    args=training_args,\n","    train_dataset=train_enc,\n","    eval_dataset=test_enc,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","# 학습 끝난 뒤\n","trainer.model.save_pretrained(\"/home/piai/PoscoAIproject/qlora-distilled-roberta-small\")\n","tokenizer.save_pretrained(\"/home/piai/PoscoAIproject/qlora-distilled-roberta-small\")\n","\n","# ===============================\n","# 9) Confusion Matrix 출력\n","# ===============================\n","out = trainer.predict(test_enc)\n","logits = out.predictions if not isinstance(out.predictions, (tuple, list)) else out.predictions[0]\n","probs = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n","y_true = out.label_ids\n","y_pred = probs.argmax(axis=-1)\n","\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"=== Confusion Matrix ===\")\n","print(cm)\n","print(\"=== Classification Report ===\")\n","print(classification_report(y_true, y_pred, digits=4))\n","\n","# 10) 추가 성능/효율 지표  (수정본)\n","# ===============================\n","import time, psutil\n","from torch.utils.data import DataLoader\n","\n","# ✅ 반드시 토크나이즈된 평가셋 사용\n","eval_loader = DataLoader(test_enc, batch_size=32, collate_fn=data_collator)\n","\n","# 배치 하나 가져오기\n","batch = next(iter(eval_loader))  # dict_keys(['input_ids','attention_mask','labels'])\n","# 라벨 제거 + 디바이스 이동\n","batch_no_label = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n","\n","# ✅ 변수명 일치: student 모델로 추론\n","student.eval()\n","with torch.no_grad():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","        torch.cuda.reset_peak_memory_stats()  # 피크 메모리 리셋\n","    t0 = time.time()\n","    _ = student(**batch_no_label)\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n","    t1 = time.time()\n","\n","B = batch_no_label[\"input_ids\"].size(0)\n","latency = (t1 - t0) / B\n","throughput = B / (t1 - t0)\n","\n","print(f\"\\n=== 추론 속도 ===\")\n","print(f\"평균 Latency: {latency*1000:.2f} ms/샘플\")\n","print(f\"Throughput : {throughput:.2f} 샘플/초\")\n","\n","# (2) 파라미터 수 — student 기준\n","total_params = sum(p.numel() for p in student.parameters())\n","trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n","print(f\"\\n=== 파라미터 수 ===\")\n","print(f\"전체 파라미터 수: {total_params/1e6:.2f} M\")\n","print(f\"학습가능 파라미터 수: {trainable_params/1e6:.2f} M\")\n","\n","# (3) 메모리 사용량\n","print(f\"\\n=== 메모리 사용량 ===\")\n","if torch.cuda.is_available():\n","    allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n","    reserved  = torch.cuda.max_memory_reserved(device)  / (1024**2)\n","    print(f\"GPU 메모리 (할당): {allocated:.2f} MB\")\n","    print(f\"GPU 메모리 (예약): {reserved:.2f} MB\")\n","else:\n","    print(f\"CPU 메모리 사용량: {psutil.Process(os.getpid()).memory_info().rss/(1024**2):.2f} MB\")\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"JngNIqtE0HK6","executionInfo":{"status":"error","timestamp":1755785414462,"user_tz":-540,"elapsed":35,"user":{"displayName":"jiwon song","userId":"09013595362968296143"}},"outputId":"3714b5ba-1bfc-4343-b7fa-b81fa4a844a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] device: cuda\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/piai/PoscoAIproject/dataset/llm_datasets_0817_non_utf8.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2548696094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# ===============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mCSV_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/home/piai/PoscoAIproject/dataset/llm_datasets_0817_non_utf8.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/piai/PoscoAIproject/dataset/llm_datasets_0817_non_utf8.csv'"]}]}]}
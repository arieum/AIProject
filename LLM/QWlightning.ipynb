{"cells":[{"cell_type":"markdown","metadata":{"id":"0KFTyw3ZaePX"},"source":["# 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsAPSloBdelx"},"outputs":[],"source":["!pip install -U transformers==4.43.3 accelerate==0.33.0 peft==0.11.1 trl==0.9.6 datasets==2.20.0 pandas==2.2.2\n","!pip install -U bitsandbytes==0.43.1\n","!pip install -U triton==2.3.0\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1755928146067,"user":{"displayName":"jiwon song","userId":"09013595362968296143"},"user_tz":-540},"id":"ldR-YUueeStS","outputId":"10b99304-568f-452b-e495-02a0dc2499d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["umount: /content/drive: not mounted.\n"]}],"source":["!sudo umount -l /content/drive\n","!rm -rf /content/drive\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hPT_WZLBaRNe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","[CLEAN] Dropped 0 rows with bad fields\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d42b718c41440608a66e624065690f0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"571f14d1182a4c47acca349197fc38c1","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6fadf44c72c14d19b6ecf551a3c81146","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd610b050e7842d3a5ebe2649cbf034b","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d633f7cf01744cc189a5202296734f4f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/660 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee72ea5c07844994830109427bd40fc5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.09G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING:bitsandbytes.cextension:Could not find the bitsandbytes CUDA binary at PosixPath('/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda126.so')\n","WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"]},{"ename":"AttributeError","evalue":"/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cquantize_blockwise_fp16_nf4","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1181355855.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 374\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1181355855.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mattn_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"flash_attention_2\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdetect_flash_attn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"eager\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 218\u001b[0;31m     student = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mSTUDENT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config_student\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3914\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3915\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 3916\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3917\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3918\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4388\u001b[0m                                 )\n\u001b[1;32m   4389\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 4390\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4391\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4392\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 938\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0;31m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0;31m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mcreate_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 217\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParams4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnb_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 324\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 289\u001b[0;31m         w_4bit, quant_state = bnb.functional.quantize_4bit(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m   1204\u001b[0m             )\n\u001b[1;32m   1205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1206\u001b[0;31m             lib.cquantize_blockwise_fp16_nf4(\n\u001b[0m\u001b[1;32m   1207\u001b[0m                 \u001b[0mget_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                 \u001b[0mget_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/cextension.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 392\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 397\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FuncPtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: /usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cquantize_blockwise_fp16_nf4"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","KD + QLoRA SFT (ChatML) for: text,label,analysis -\u003e {\"is_phishing\": bool, \"reason\": str}\n","- Teacher: Qwen2.5-3B-Instruct (지식 증류용 soft target 제공, 학습 X)\n","- Student: Qwen2.5-1.5B-Instruct (4bit + QLoRA 학습)\n","- assistant 구간만 Loss (Hard CE + Soft KD)\n","- ChatML 안전 생성 / response_template 자동 추출\n","- 기존 설정(4bit, NEFTune, FlashAttention2 등) 유지\n","\"\"\"\n","\n","import os, random, json, re\n","from typing import Dict, Any\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import (\n","    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",")\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","from trl import DataCollatorForCompletionOnlyLM\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ===== 사용자 수정 =====\n","CSV_PATH    = \"/content/drive/MyDrive/KDH/dataset/llm_datasets_0817_flow_utf8.csv\"  # text,label,analysis\n","OUTPUT_DIR  = \"/content/drive/MyDrive/KDH/llmlightning/lora_kd_qwen1p5b_from_3b\"\n","TEACHER_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n","STUDENT_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","\n","EPOCHS        = 2\n","BATCH_SIZE    = 1\n","GRAD_ACCUM    = 16\n","LR            = 1e-4\n","WARMUP_RATIO  = 0.10\n","WEIGHT_DECAY  = 0.10\n","MAX_GRAD_NORM = 0.30\n","MAX_LEN       = 1024\n","VAL_RATIO     = 0.1\n","SEED          = 42\n","\n","# KD 하이퍼파라미터\n","KD_ALPHA       = 0.5      # Hard CE 가중치 (나머지 1-KD_ALPHA가 Soft KD 가중치)\n","KD_TEMPERATURE = 2.0\n","\n","# LoRA 설정 (Qwen 계열)\n","LORA_R         = 32\n","LORA_ALPHA     = 64\n","LORA_DROPOUT   = 0.10\n","TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n","\n","# 기타\n","NEFTUNE_NOISE_ALPHA = 5           # SFTTrainer 기능이지만, 여기선 참고만 (KD 커스텀이라 미사용)\n","TEACHER_4BIT        = False       # 메모리 부족하면 True로\n","# ======================\n","\n","\n","def set_seed(s: int):\n","    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n","\n","\n","def bf16_supported() -\u003e bool:\n","    try:\n","        return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n","    except Exception:\n","        return False\n","\n","\n","def detect_flash_attn2() -\u003e bool:\n","    try:\n","        import flash_attn  # noqa\n","        return True\n","    except Exception:\n","        return False\n","\n","\n","def get_tokenizer(model_name: str):\n","    try:\n","        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n","    except Exception:\n","        tok = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n","    if tok.pad_token is None:\n","        tok.pad_token = tok.eos_token\n","    return tok\n","\n","\n","def build_user_content(text: str) -\u003e str:\n","    \"\"\"학습 user 메시지 (분석 지시 포함, JSON 외 금지)\"\"\"\n","    lines = [\n","        \"다음 대화를 보고 순수 JSON으로만 답해라.\",\n","        '스키마: {\"is_phishing\": true|false, \"reason\": \"한국어 1~2문장\"}',\n","        \"- JSON 이외의 텍스트 금지.\",\n","        \"\",\n","        \"[대화]\",\n","        str(text).strip()\n","    ]\n","    return \"\\n\".join(lines)\n","\n","\n","def apply_chat_template_text(tokenizer, system: str, user_content: str, assistant_json: Dict[str, Any]) -\u003e str:\n","    messages = [\n","        {\"role\": \"system\", \"content\": system},\n","        {\"role\": \"user\", \"content\": user_content},\n","        {\"role\": \"assistant\", \"content\": json.dumps(assistant_json, ensure_ascii=False)},\n","    ]\n","    return tokenizer.apply_chat_template(messages, tokenize=False)\n","\n","\n","def get_response_template_from_tokenizer(tokenizer) -\u003e str:\n","    tmpl = tokenizer.apply_chat_template(\n","        [{\"role\": \"assistant\", \"content\": \"\"}], tokenize=False, add_generation_prompt=False\n","    )\n","    if \"\u003c|im_end|\u003e\" in tmpl:\n","        tmpl = tmpl.split(\"\u003c|im_end|\u003e\")[0]\n","    return tmpl\n","\n","\n","def main():\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","    set_seed(SEED)\n","\n","    # ============= 1) 데이터 로드/정리 (text,label,analysis) =============\n","    df = pd.read_csv(CSV_PATH)\n","    assert all(c in df.columns for c in [\"text\", \"label\", \"analysis\"]), \"CSV에 text,label,analysis 컬럼이 필요합니다.\"\n","\n","    # text / analysis 정리\n","    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n","    df[\"analysis\"] = df[\"analysis\"].astype(str).str.strip()\n","\n","    # label 표준화\n","    label_map = {\"true\":1,\"false\":0,\"t\":1,\"f\":0,\"yes\":1,\"no\":0,\"y\":1,\"n\":0,\"보이스피싱\":1,\"정상\":0}\n","    def norm_label(x):\n","        if pd.isna(x): return np.nan\n","        s = str(x).strip().lower()\n","        if s in (\"\", \"nan\", \"none\"): return np.nan\n","        if s in label_map: return label_map[s]\n","        try:\n","            v = float(s)\n","            return 1 if np.isfinite(v) and v \u003e= 0.5 else 0\n","        except:\n","            return np.nan\n","\n","    df[\"label\"] = df[\"label\"].apply(norm_label)\n","    df[\"label\"] = df[\"label\"].replace([np.inf, -np.inf], np.nan)\n","\n","    # reason 길이 간단 제한(너무 길면 2문장 수준으로 잘라주기)\n","    def re_split_sent(s: str):\n","        # 마침표/느낌표/물음표 OR 닫는 괄호 뒤 공백에서 분리\n","        return re.split(r'(?\u003c=[.!?])\\s+|(?\u003c=\\))\\s+', s)\n","\n","    def trim_reason(s: str) -\u003e str:\n","        s = s.replace(\"\\n\", \" \").strip()\n","        parts = [p.strip() for p in re_split_sent(s)]\n","        return \" \".join(parts[:2]) if parts else s\n","\n","    df[\"analysis\"] = df[\"analysis\"].apply(trim_reason)\n","\n","    before = len(df)\n","    df = df.dropna(subset=[\"text\",\"label\",\"analysis\"]).copy()\n","    df[\"label\"] = df[\"label\"].astype(int)\n","    after = len(df)\n","    print(f\"[CLEAN] Dropped {before - after} rows with bad fields\")\n","\n","    # 중복 제거\n","    df = df.drop_duplicates(subset=[\"text\",\"analysis\"]).reset_index(drop=True)\n","\n","    train_df, val_df = train_test_split(\n","        df, test_size=VAL_RATIO, random_state=SEED, stratify=df[\"label\"]\n","    )\n","\n","    # ============= 2) ChatML 샘플 생성 (reason = analysis 사용) =============\n","    SYSTEM = (\n","        \"너는 보이스피싱 대화를 감별하는 분석가다. \"\n","        \"입력 대화를 보고 **순수 JSON만** 출력한다. \"\n","        '스키마: {\"is_phishing\": true|false, \"reason\": \"한국어 1~2문장\"} '\n","        \"JSON 이외의 텍스트는 절대 출력하지 마라.\"\n","    )\n","\n","    # tokenizer (공유)\n","    tokenizer = get_tokenizer(STUDENT_MODEL)  # Student 기준으로 패딩/토크나이즈(동일 ChatML)\n","\n","    def row_to_text(row) -\u003e Dict[str, Any]:\n","        user_content = build_user_content(row[\"text\"])\n","        target = {\n","            \"is_phishing\": bool(int(row[\"label\"]) == 1),\n","            \"reason\": row[\"analysis\"][:400]  # 혹시 모를 과도한 길이 방지\n","        }\n","        chat_text = apply_chat_template_text(tokenizer, SYSTEM, user_content, target)\n","        return {\"text\": chat_text}\n","\n","    train_records = [row_to_text(r) for _, r in train_df.iterrows()]\n","    val_records   = [row_to_text(r) for _, r in val_df.iterrows()]\n","\n","    train_ds = Dataset.from_list(train_records)\n","    val_ds   = Dataset.from_list(val_records)\n","\n","    # ============= 3) Collator (assistant 구간만 Loss 마스킹) =============\n","    ASSISTANT_PREFIX = get_response_template_from_tokenizer(tokenizer)\n","    collator = DataCollatorForCompletionOnlyLM(\n","        response_template=ASSISTANT_PREFIX, tokenizer=tokenizer, mlm=False\n","    )\n","\n","    # ============= 4) 모델 로드 =============\n","    # ---- Student: 1.5B + QLoRA(4bit) ----\n","    bnb_config_student = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.bfloat16 if bf16_supported() else torch.float16,\n","    )\n","    attn_impl = \"flash_attention_2\" if detect_flash_attn2() else \"eager\"\n","\n","    student = AutoModelForCausalLM.from_pretrained(\n","        STUDENT_MODEL,\n","        quantization_config=bnb_config_student,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n","        attn_implementation=attn_impl,\n","    )\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    student.gradient_checkpointing_enable()\n","    student.config.use_cache = False\n","    student = prepare_model_for_kbit_training(student)\n","\n","    lora_cfg = LoraConfig(\n","        r=LORA_R,\n","        lora_alpha=LORA_ALPHA,\n","        lora_dropout=LORA_DROPOUT,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=TARGET_MODULES,\n","    )\n","    student = get_peft_model(student, lora_cfg)\n","    student.print_trainable_parameters()\n","\n","    # ---- Teacher: 3B (KD용, 학습 X) ----\n","    if TEACHER_4BIT:\n","        bnb_config_teacher = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_compute_dtype=torch.bfloat16 if bf16_supported() else torch.float16,\n","        )\n","        teacher = AutoModelForCausalLM.from_pretrained(\n","            TEACHER_MODEL,\n","            quantization_config=bnb_config_teacher,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            attn_implementation=\"eager\",  # 안정성 위해 eager 권장\n","        )\n","    else:\n","        teacher = AutoModelForCausalLM.from_pretrained(\n","            TEACHER_MODEL,\n","            device_map=\"auto\",\n","            trust_remote_code=True,\n","            torch_dtype=torch.bfloat16 if bf16_supported() else torch.float16,\n","            attn_implementation=\"eager\",\n","        )\n","    teacher.eval()\n","    for p in teacher.parameters():\n","        p.requires_grad = False\n","\n","    # ============= 5) KD 전용 Trainer =============\n","    class KDSFTTrainer(Trainer):\n","        \"\"\"\n","        - inputs: DataCollatorForCompletionOnlyLM가 만든 dict (input_ids, attention_mask, labels)\n","        - Hard CE: labels != -100 (assistant 구간)\n","        - Soft KD: teacher/student 분포 KLDiv on assistant 구간 (temperature scaling)\n","        \"\"\"\n","        def __init__(self, *args, teacher_model=None, kd_alpha=0.5, kd_temperature=2.0, **kwargs):\n","            super().__init__(*args, **kwargs)\n","            self.teacher = teacher_model.eval()\n","            self.kd_alpha = kd_alpha\n","            self.kd_temperature = kd_temperature\n","            self.ce_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","\n","        @torch.no_grad()\n","        def _teacher_logits(self, inputs):\n","            # teacher 디바이스로 이동\n","            tdev = next(self.teacher.parameters()).device\n","            tinp = {k: v.to(tdev) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n","            out = self.teacher(**tinp)\n","            return out.logits\n","\n","        def compute_loss(self, model, inputs, return_outputs=False):\n","            labels = inputs.get(\"labels\")\n","            outputs_s = model(**inputs)\n","            logits_s = outputs_s.logits  # [B, T, V]\n","\n","            # Hard CE (assistant 구간)\n","            loss_ce = self.ce_loss(\n","                logits_s.view(-1, logits_s.size(-1)),\n","                labels.view(-1)\n","            )\n","\n","            # Soft KD (teacher 분포)\n","            with torch.no_grad():\n","                logits_t = self._teacher_logits(inputs)  # [B, T, V]\n","\n","            # assistant 구간 마스크\n","            mask = labels.ne(-100)  # [B, T]\n","            if mask.any():\n","                T = self.kd_temperature\n","                # 마스크된 위치만 추출 → [N, V]\n","                s = (logits_s / T)[mask]\n","                t = (logits_t / T)[mask]\n","                loss_kd = F.kl_div(\n","                    F.log_softmax(s, dim=-1),\n","                    F.softmax(t, dim=-1),\n","                    reduction=\"batchmean\"\n","                ) * (T * T)\n","            else:\n","                loss_kd = torch.zeros((), device=logits_s.device)\n","\n","            loss = self.kd_alpha * loss_ce + (1.0 - self.kd_alpha) * loss_kd\n","            return (loss, outputs_s) if return_outputs else loss\n","\n","    # ============= 6) 학습 설정 =============\n","    use_bf16 = bf16_supported()\n","    train_args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        num_train_epochs=EPOCHS,\n","        per_device_train_batch_size=BATCH_SIZE,\n","        per_device_eval_batch_size=1,\n","        gradient_accumulation_steps=GRAD_ACCUM,\n","        learning_rate=LR,\n","        lr_scheduler_type=\"cosine\",\n","        warmup_ratio=WARMUP_RATIO,\n","        weight_decay=WEIGHT_DECAY,\n","        max_grad_norm=MAX_GRAD_NORM,\n","        logging_steps=20,\n","        save_steps=500,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=500,\n","        save_total_limit=2,\n","        bf16=use_bf16,\n","        fp16=(not use_bf16),\n","        gradient_checkpointing=True,\n","        optim=\"paged_adamw_8bit\",\n","        group_by_length=True,\n","        dataloader_pin_memory=True,\n","        report_to=\"none\",\n","        ddp_find_unused_parameters=False if torch.cuda.device_count() \u003e 1 else None,\n","        max_steps=-1,\n","        save_safetensors=True,\n","    )\n","\n","    # ============= 7) Trainer 생성/학습/저장 =============\n","    trainer = KDSFTTrainer(\n","        model=student,\n","        tokenizer=tokenizer,\n","        args=train_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        data_collator=collator,\n","        teacher_model=teacher,\n","        kd_alpha=KD_ALPHA,\n","        kd_temperature=KD_TEMPERATURE,\n","    )\n","\n","    trainer.train()\n","    trainer.model.save_pretrained(OUTPUT_DIR)\n","    tokenizer.save_pretrained(OUTPUT_DIR)\n","    print(f\"[OK] KD + QLoRA LoRA saved to: {OUTPUT_DIR}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{"id":"UQYr9ZJPalfz"},"source":["# 추론"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTBoBl-yadxU"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import json, re, torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","# ========= 모델 경로 =========\n","BASE = \"Qwen/Qwen2.5-3B-Instruct\"\n","LORA = \"/content/drive/MyDrive/KDH/LLM/lora_label_analysis\"  # 네가 방금 학습한 LoRA 경로\n","\n","# ========= 로드 =========\n","tok = AutoTokenizer.from_pretrained(BASE, use_fast=True, trust_remote_code=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","base = AutoModelForCausalLM.from_pretrained(\n","    BASE, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",").eval()\n","model = PeftModel.from_pretrained(base, LORA).eval()\n","\n","# ========= 프롬프트 (학습 스키마와 정확히 일치) =========\n","SYSTEM = (\n","    \"너는 보이스피싱 탐지 전문가다. 반드시 순수 JSON 한 덩어리만 출력한다. \"\n","    '스키마는 {\"is_phishing\": true|false, \"reason\": \"한국어 1~2문장\"} 이다. '\n","    \"JSON 밖의 텍스트/주석/라벨/마크다운은 금지한다.\"\n",")\n","\n","USER_TMPL = (\n","    \"다음 대화를 읽고 실제 공공기관 정상 절차와 비교하여 왜 다른지 요약하고, \"\n","    \"수법(기관 사칭/안전계좌 유도/환급 미끼/원격앱 유도 등)을 근거로 보이스피싱 여부를 판단하라. \"\n","    '오직 {\"is_phishing\": true|false, \"reason\": \"한국어 1~2문장\"} 형태의 JSON만 출력하라.\\n'\n","    \"대화:\\n\\\"\\\"\\\"\\n{TEXT}\\n\\\"\\\"\\\"\"\n",")\n","\n","# ========= JSON 회수(견고) =========\n","def _extract_balanced_json(text: str):\n","    in_str = False; esc = False; depth = 0; start = -1\n","    for i, ch in enumerate(text):\n","        if ch == '\"' and not esc: in_str = not in_str\n","        esc = (ch == '\\\\') and not esc\n","        if in_str: continue\n","        if ch == '{':\n","            if depth == 0: start = i\n","            depth += 1\n","        elif ch == '}':\n","            if depth \u003e 0:\n","                depth -= 1\n","                if depth == 0 and start != -1:\n","                    block = text[start:i+1]\n","                    try:\n","                        return json.loads(block)\n","                    except Exception:\n","                        pass\n","    raise ValueError(\"No valid JSON object found\")\n","\n","_SENT_SPLIT = re.compile(r'(?\u003c=[.!?])\\s+|(?\u003c=\\))\\s+')\n","\n","def _postprocess(js: dict) -\u003e dict:\n","    out = {}\n","    out[\"is_phishing\"] = bool(js.get(\"is_phishing\", False))\n","    reason = str(js.get(\"reason\", \"\")).strip()\n","    if reason:\n","        parts = [p.strip() for p in _SENT_SPLIT.split(reason) if p.strip()]\n","        reason = \" \".join(parts[:2])  # 최대 2문장\n","    out[\"reason\"] = reason\n","    return out\n","\n","# ========= 추론 =========\n","def infer(text: str, max_new_tokens: int = 160, deterministic: bool = True) -\u003e dict:\n","    user = USER_TMPL.replace(\"{TEXT}\", text)\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYSTEM},\n","        {\"role\": \"user\", \"content\": user},\n","    ]\n","\n","    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    with torch.no_grad():\n","        out_ids = model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=not deterministic,   # 기본은 일관성 우선\n","            top_p=0.9,\n","            temperature=0.6,\n","            repetition_penalty=1.08,\n","            no_repeat_ngram_size=4,\n","            eos_token_id=tok.eos_token_id,\n","        )\n","\n","    gen = tok.decode(out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n","    try:\n","        js = _extract_balanced_json(gen)\n","    except Exception:\n","        return {\"error\": gen}\n","\n","    # 스키마 고정 + 2문장 컷\n","    js = {k: js[k] for k in js.keys() if k in (\"is_phishing\", \"reason\")}\n","    return _postprocess(js)\n","\n","# ========= 사용 예시 =========\n","if __name__ == \"__main__\":\n","    text = (\n","        \"신용카드와 대포통장을 압수하는 과정에서 귀하의 명의로 된 농협과 하나은행 통장 두 개가 발견되어 연락드렸습니다. 이 통장들에 대해 알고 계신 사실이 있으신가요?\"\n","    )\n","    print(infer(text))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FanxtMNZamf4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPF2OoClUyyAm1nbfpaKbjJ","gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}